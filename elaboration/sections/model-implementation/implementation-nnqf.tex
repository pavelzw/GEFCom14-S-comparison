\section{Nearest Neighbor Quantile Filters}
\label{sec:implementation-nnqf}

Since the NNQF method is only a preprocessing step for the target values, 
we still need to decide which model we want to use for fitting the 
conditional distribution function. 
\Textcite{Ordiano2019} tried fitting each of the \(99\) quantiles 
with a polynomial of maximum degree \(1\) to \(4\) or a multi layer perceptron 
with \(6\) or \(10\) hidden neurons. Since the multi layer perceptrons lands 
significantly better results, I will focus on them. 

The implementation of the model is done through pyWATTS, like in the SQF-RNN case.

Because the data is a time series, timepoints that are close are correlated. 
That's why we not only take the predictor value \(x_n \in \R^D\) of time point \(n\) 
but also \(x_{n-1}, \ldots, x_{n-H+1}\) as predictor values, where \(H\) is the number of lags.
All in all, we want to fit a function \(\func{f_q}{\R^{D\times H}}{\R}\), 
where \( f(x_n, \ldots, x_{n-H+1}; \theta_{(q)})\) is the conditional \(q\)-quantile of the target value \(Y_n\).

In order to achieve this lagging, pyWATTS contains a \texttt{Sampler} class
that transforms the data in a way that afterwards, each timepoint contains the 
predictor data of the previous \(H\) timepoints.

\Textcite{Ordiano2019} uses a different neural network with \(6\) or \(10\) hidden nodes for each quantile, 
which is computationally more expensive than training one neural network with 
one hidden layer with \(50\) nodes for \(99\) outputs. 
Both methods perform approximately the same. \todo{add numbers to show that} 

Since implementing a model in pyWATTS with a variable number of neural networks is currently 
not easily possible, we will use a single neural network to approximate the quantiles.

In order to remove quantile crossing, \Textcite{Ordiano2019} postprocesses the conditional quantiles:
\[ \hat{y}_{(q)} = \begin{cases}
    \max\set{ f(x; \theta_{(q)}), 0 }, &\text{if } q = 0.01, \\
    \max\set{ f(x; \theta_{(q)}), f(x; \theta_{(q-0.01)}) }, &\text{else.}
\end{cases}\]

In this thesis, I used another approach: 
I sort all estimated conditional quantiles \(\set{ f(x; \theta_{(q)}) \;|\; q\in \set{0.01, \ldots, 0.99} }\) 
and set \(\hat{y}_{(q)}\) as the \((q\cdot 100)\)-th entry of the sorted list. This results in a significant performance improvement 
in comparison to taking the maximum. \todo{add numbers to show significant improvement!}

After that, the pinball loss is calculated the same way as in the SQF-RNN case.