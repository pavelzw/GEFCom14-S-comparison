\section{Quantile Regression Forests}
\label{sec:implementation-qrf}

A Python implementation for Quantile Regression Forests can 
be found in the doubt package\footnote{\url{https://github.com/saattrupdan/doubt}}.

First, the data is read from the \texttt{.csv} files. 
Afterwards, the cumulated columns (SSRD, STRD, TSR) are decumulated and then normalized 
as described in Section \ref{sec:data-preprocessing}.
Then, the \texttt{QuantileRegressionForest} is wrapped into a pipeline stage in order for 
it to be trained.
After training and prediction, the results are evaluated 
using the pinball loss scoring rule for each time step and zone.

The hyperparameters of Quantile Regression Forests are very similar to the ones 
of conventional Random Forests. We can choose the number of trees in the forest, 
the splitting criterion (mean squared error or mean absolute error), 
the splitting strategy (best split or best random split
\footnote{The best random split selects the best split of all features while best random split 
randomly selects features, calculates their performance and selects the best split 
among these randomly selected features.}) 
and the number of features to consider when looking for the best split. 
We can also change the shape of the trees: 
the maximum depth, minimum number of samples required to split a node, the minimum number of samples per leaf and 
the maximum number of leaf nodes can all be adjusted.
The parameters after tuning as well as the optimization space 
are shown in Table \ref{table:qrf-hyperparameters}; the dash indicates that we don't optimize over this value.
Most of the losses are in the range \([0.18, 0.24]\). 
The default configuration resulted in a loss of \(0.2\) so 
the optimization results in an improvement of around \(10\%\).

\begin{table}[h!]%
    \caption{QRF Hyperparameters}
    \label{table:qrf-hyperparameters}
    \rowcolors{2}{white}{gray!25}
    \centering
    \footnotesize
    \begin{tabular}{lll}
    \toprule \noalign{\smallskip}
    \tableheads Hyperparameter & \tableheads Optimization space & \tableheads Final value \\ 
    \midrule
    Number of trees                             & \(\set{50, 100, 150, 200}\)    & \(100\)            \\
    Splitting criterion                         & --                             & mean squared error \\
    Splitting strategy                          & --                             & best split         \\
    Maximum number of features for split        & \(\set{1, 2, 3, 
                                                  \text{number of features}}\)   & \(3\)              \\
    Maximum depth                               & \(\set{5, 10, 20, 30, 40, 50, 
                                                  \text{any size}}\)             & \(10\)             \\
    Minimum number of samples required to split & \(\set{2, 4, 6, 8, 10}\)       & \(4\)              \\
    Minimum number of samples per leaf          & \(\set{1, 2, 4, 8}\)           & \(8\)              \\
    Maximum number of leaves                    & \(\set{50, 100, 200, 300, n}\) & \(50\)             \\
    \bottomrule
    \end{tabular}
\end{table}