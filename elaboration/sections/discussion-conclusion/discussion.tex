\section{Discussion}
\label{sec:discussion}

In this section, we discuss the results, compare the different 
models and look at potential weaknesses of the competition. 

Looking at Figure \ref{fig:pinball-loss}, we see that NNQF and QRF perform very similar. 
In the months from October to Febuary, i.e., the summer months since 
Australia is located in the southern hemisphere, we can see that 
the SQF-RNN model performs noticably better than the NNQF and QRF models. 
One explanation could be that the NNQF and QRF models only focus on solar 
radiation but not on total cloud cover.  
In the summer months, the solar energy is influenced in a large part by 
how many clouds are in the sky. Figure \ref{fig:feature-importance} shows us that 
the SQF-RNN model focuses more on total cloud cover. Therefore, 
it performs better in these months. 
We can also observe that the SQF-RNN model always outperforms the DeepAR model. 
Thus we can conclude that the Student's \(t\)-distribution is not the right fit 
and spline quantile functions model the distribution better. 

When looking at the energy score in Figure \ref{fig:energy-score}, 
we can see that the SQF-RNN model outperforms the NNQF model as well as QRF.
One reason why the SQF-RNN works better here is that it takes 
the time series attributes into account because of its RNN structure. 
The NNQF model only uses lag features while the QRF model doesn't look 
at the previous values at all, it only looks at the current time point.

When comparing the SQF-RNN and the DeepAR model with 
the Student's \(t\)-distribution in Figure \ref{fig:pinball-loss} and 
Figure \ref{fig:energy-score}, we can see that the DeepAR model 
always performs worse than the SQF-RNN model. 
Here we can see that the nonparametric approach is better than assuming 
that the target variable ist distributed by an arbitrary distribution. 

As discussed in chapter \ref{ch:evaluation}, 
the QRF model's predictions are capped 
at the top (Figure \ref{fig:predictions-qrf}). 
This is because of the QRF's predictive nature: it generates 
the prediction directly from the previous target data 
and thus cannot predict values 
that are higher than the maximal values of the training set. 
The other two models don't suffer from this because they use 
neural networks to predict the target distribution.

The PIT histograms tell us if the forecasting method is probabilistically 
calibrated or if it is probabilistically calibrated or 
if it is over- or underdispersed. 
Figure \ref{fig:pit} indicate that the QRF and NNQF model are 
probabilistically calibrated while the PIT histograms of SQF-RNN 
and DeepAR model are skewed and thus their predictive distribution is 
biased in their location.

When we look at the PIT histograms by each hour 
for quantile regression forests in Figure \ref{fig:pit-qrf-by-hour} 
we can observe that the predictions for the night are underdispersed. 
Since the solar plants don't produce energy during the night 
it makes sense to just predict \(0\) for the target variable when there is 
no sunshine.

The NNQF model also suffers a bit from this problem. We can see 
in Figure \ref{fig:pit-nnqf-by-hour} in the time frame 22:00-00:00 that 
there is a high density at \(0\) indicating that the forecast 
was often too high. This can be fixed like in the proposition above.

The hourly PIT histograms for the SQF-RNN and DeepAR model in 
Figure \ref{fig:pit-deepar-by-hour} and Figure \ref{fig:pit-sqf-rnn-by-hour} 
both indicate the same as mentioned above: both predictions are biased 
in their location. One solution would be postprocessing by shifting the 
predictions so that the PITs are approximately symmetric. 

The competition wants to mimic real time solar energy forecasting with 
a \(\SI{24}{\hour}\) forecast horizon. In this scenario, the energy data of 
the previous days would be available. Since the competition leader want to 
prevent overfitting the data, the target variables are not available for 
the entire month. This is one reason why the SQF-RNN model doesn't perform 
as expected. 
It is the most complicated of the three but has the worst results. 
In order to compare the model fairly with the other competitors, 
we implemented it under competition conditions. A more realistic approach would be 
providing the model with the target data and evaluating it this way.