\section{Nearest Neighbor Quantile Filters}
\label{sec:nnqf}

\Textcite{Ordiano2019} proposed a method for probabilistic 
energy forecasting using quantile regression based on a \gls{nnqf}. 
The method works as follows: first, the training set is modified 
by using the Nearest Neigbor Quantile Filters so that 
the training data directly represents a probabilistic distribution. 
Then, a regression model like an artificial neural network can 
train on this modified data set and learn the quantile function.

The preprocessing of the NNQF method includes multiple steps. 
Firstly, we search the \(k\) nearest neighbors 
for every \(x_i\).
The distance metric can be any distance metric on \(\R^D\), 
the euclidean metric is used often.
Let \(J \subset \set{1, \ldots, n}\) be the indices of 
those nearest neighbors. 
The probabilistic distribution of \(y_i\) can be approximated 
by calculating the empirical quantile \(\tilde{y}_{(q),i}\) of 
\(\set{y_j \;|\; j\in J}\) for each \(q \in \set{0.01, \ldots, 0.99}\). 

After repeating this procedure for each entry in the time series, 
we get vectors 
\[ \tilde{y}_{(q)} = \begin{pmatrix}
    \tilde{y}_{(q), 1} \\ 
    \vdots \\
    \tilde{y}_{(q), n}
\end{pmatrix} \]
that form the modified training set combined with the 
predictors \(X = (x_1, \ldots, x_n)\).

Because we work with a time series, adjacent points are correlated. 
That's the reason why we use lag features: 
instead of only \(x_i\), we are using \(x_i, \ldots, x_{i-H+1}\) to 
predict the target data. \(H\) is called lag size.

With the modified training set, one can now train the regression model 
for each quantile and fit the function 
\[ f_\theta(x_i, \ldots, x_{i-H+1}) = \tilde{y}_{(q), i}. \]
Common examples are polynomial regression or 
artificial neural networks. 

This method has three main advantages: 
\begin{enumerate}
    \item The technique for the quantile regression is not specified, 
    any technique can be used,
    \item the calculation of the nearest neighbors and the modified 
    training set only needs to be done once, you can save time when 
    using multiple quantile regressions. 
    \item The original dataset does not need to be saved afterwards, 
    we only need the weights of the regression model for predicting.
\end{enumerate}

In comparison to most other \(k\)-Nearest Neighbors quantile 
regression techniques, the nearest neighbors are only calculated once 
and then the regression model is trained on the modified training data. 
A regular \(k\)-Nearest Neighbor quantile regression algorithm 
calculates the nearest neighbors every time when a forecast is conducted 
(cf. \Textcite{Ma2015}, p. 3 ff) which is computationally more expensive in the long run.