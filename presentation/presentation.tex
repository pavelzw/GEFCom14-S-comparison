\documentclass[10pt,aspectratio=169]{beamer}

\usetheme[progressbar=frametitle]{metropolis}

\include{preamble}

\title{Non-Parametric Machine Learning Models for Solar Energy Forecasting}
% \subtitle{}
\date{16.07.2021}
\author{Pavel Zwerschke}
\institute{Karlsruhe Institute of Technology}
\titlegraphic{\hfill\includegraphics[height=1.5cm]{logos/kitlogo_en_cmyk}}

\usepackage[style=authoryear, maxcitenames=2, bibencoding=inputenc]{biblatex}
\bibliography{bibliography.bib}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents%[hideallsubsections]
\end{frame}

\begin{frame}{Introduction}
    \begin{itemize}
        \item Solar energy generation is characterized by fluctuations due to uncertainty of the weather
        \item Uncertainty should be quantified through a probabilistic forecast
        \begin{itemize}
            \item Probabilistic forecasts for solar energy generation are underdeveloped
            \item[\(\leadsto\)] We want to compare different machine learning based forecasting models on solar power
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Model descriptions}

\begin{frame}{NNQF [\cite{Ordiano2020}]}
    \begin{center}
        \includegraphics{plots/nnqf_approach.pdf}
    \end{center}
    \begin{itemize}
        \item Let \(x_1, \ldots, x_n \in \R^D\) be the predictors and \(y_1, \ldots, y_n\in \R\) the target values.
        \item Calculate approximate quantiles of \(y_i\):
        \begin{itemize}
            \item Find \(N\) nearest neighbors of \(x_i\): \(\set{x_{i_1}, \ldots, x_{i_N}}\)
            \item Calculate the empirical quantiles \(y_{(0.01)}, \ldots, y_{(0.99)}\) from \(\set{y_{i_1}, \ldots, y_{i_N}}\)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{NNQF [\cite{Ordiano2020}]}
    \begin{itemize}
        \item After modification of training set, a data mining technique is used for learning the map \(f(x) = (y_{(0.01)}, \ldots, y_{(0.99)})\).
        \item High correlation of adjacent data points
        \begin{itemize}
            \item[\(\leadsto\)] use lag features: don't just use \(x_i\) for prediction of \(y_i\), but also 
            \(x_{i-1}, \ldots, x_{i-H+1}\)
            \item regression model now predicts \( \P(y_i | x_i, \ldots, x_{i-H+1}) \)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Advantages of NNQF [\cite{Ordiano2020}]}
    \begin{itemize}
        \item the regression technique is not specified, any technique can be used
        \begin{itemize}
            \item in the classical quantile regression techniques and QRF, the algorithm needs to be specified
        \end{itemize}
        \item nearest neighbor calculation only needs to be done once
        \begin{itemize}
            \item nearest neighbor quantile regression calculates the nearest neighbors for each prediction
        \end{itemize}
        \item the original dataset does not need to be saved
    \end{itemize}
\end{frame}

\begin{frame}{QRF [\cite{Meinshausen2006}]}
    \begin{itemize}
        \item Use bagging to produce \(k\) trees from training set \(x_1, \ldots, x_n \in \R^D\) and \(y_1, \ldots, y_n \in \R\)
        \item For \(x\in \R^D\), we want to predict the distribution \(\P(Y | X=x)\)
        \begin{itemize}
            \item Calculate \(\hat{y}_1, \ldots, \hat{y}_k\) from the trees 
            \item Calculate the empirical quantiles \(\hat{y}_{(q)}\) of \(\set{\hat{y}_1, \ldots, \hat{y}_k}\) for any \(q \in (0,1)\)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{DeepAR -- Training [\cite{Salinas2020}]}
    \begin{center}
        \input{plots/deepar_training}
    \end{center}
    \begin{itemize}
        \item Autoregressive Recurrent Neural Network with probabilistic output
        \item \(x_t\) and \(z_{t-1}\) form with \(\boldsymbol{h}_{t-1}\) the new network output \(\boldsymbol{h}_t\)
        which is used to compute the likelihood \(\P(z_t | \boldsymbol{h}_t)\)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{DeepAR -- Predicting [\cite{Salinas2020}]}
    \begin{center}
        \input{plots/deepar_predicting}
    \end{center}
    \begin{itemize}
        \onslide<2->{\item Generate \(\tilde{z}_t \sim \P(\cdot | \boldsymbol{h}_t)\) and use it in the next step as input}
    \end{itemize}
\end{frame}

\begin{frame}{SQF-RNN [\cite{Gasthaus2019}]}
    \begin{columns}
    \begin{column}{0.6\textwidth}
    \begin{itemize}
        \item Main difference to DeepAR is output distribution
        \begin{itemize}
            \item Given by monotonously increasing linear splines: 
            \[ s(x; \gamma, b, d) = \gamma + \sum_{l=0}^L b_l (x - d_l)_+ \]
            \item Arbitrary distribution can be fit \(\leadsto\) no assumption on distribution
        \end{itemize}
    \end{itemize}
    \end{column}

    \begin{column}{0.35\textwidth}
        \input{plots/spline}
    \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Summary}
    \begin{itemize}
        \item NNQF
        \begin{itemize}
            \item Preprocessing with nearest neighbor filters \(\leadsto\) regression for preprocessed data
        \end{itemize}
        \item QRF
        \begin{itemize}
            \item Random Forests \(\leadsto\) take empirical quantiles of result
        \end{itemize}
        \item SQF-RNN
        \begin{itemize}
            \item Deep neural network with autoregressive input
            \item Target distribution modeled by spline quantile functions
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Plots}
    \begin{columns}
        \begin{column}{0.33\textwidth}
            \includegraphics[width=\textwidth]{plots/nnqf_plot_9.pdf}
            \begin{center}
                NNQF
            \end{center}
        \end{column}
        \begin{column}{0.33\textwidth}
            \includegraphics[width=\textwidth]{plots/qrf_plot_9.pdf}
            \begin{center}
                QRF
            \end{center}
        \end{column}
        \begin{column}{0.33\textwidth}
            \includegraphics[width=\textwidth]{plots/sqf_rnn_plot_9.pdf}
            \begin{center}
                SQF-RNN
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\section{Comparison}

\begin{frame}{Feature importance}
    \begin{center}
        \input{plots/feature_importance}
    \end{center}
\end{frame}

\begin{frame}[fragile]{Pinball loss}
    \begin{columns}
    \begin{column}{0.5\textwidth}
        \begin{flushright}
            \input{plots/pinball_loss}
        \end{flushright}
    \end{column}
    \begin{column}{0.5\textwidth}
        Mean losses:
        \begin{description}
            \item[\textcolor{TolDarkBlue}{NNQF}] \(0.01940\), place 16/24
            \item[\textcolor{TolLightBrown}{QRF}] \(0.02015\), place 17/24
            \item[\textcolor{TolLightGreen}{SQF-RNN}] \(0.02041\), place 18/24
            \item[\textcolor{TolDarkBrown}{DeepAR}] \(0.02437\), place 20/24
        \end{description}
    \end{column}
    \end{columns}
    
    \begin{itemize}
        \item NNQF and QRF perform very similar
        \item In the summer months (southern hemisphere!), SQF-RNN performs noticably better than NNQF and QRF models
        \begin{itemize}
            \item[\(\leadsto\)] NNQF and QRF only focus on solar radiation, not on total cloud cover
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Energy score}
    \begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{flushright}
        \input{plots/energy_score}
    \end{flushright}
    \end{column}
    \begin{column}{0.5\textwidth}
    Mean losses:
    \begin{description}
        \item[\textcolor{TolDarkBlue}{NNQF}] \(0.36736\)
        \item[\textcolor{TolLightBrown}{QRF}] \(0.36759\)
        \item[\textcolor{TolLightGreen}{SQF-RNN}] \(0.36644\)
        \item[\textcolor{TolDarkBrown}{DeepAR}] \(0.44235\)
    \end{description}
    \end{column}
    \end{columns}
    
    \begin{itemize}
        \item SQF-RNN \(\succ\) NNQF \(\approx\) QRF in the energy score
        \begin{itemize}
            \item Energy score takes time series attributes into account, SQF-RNN is better at that due to the RNN structure
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item NNQF is best on average
        \item NNQF is slightly better than QRF
        \item SQF-RNN is noticably better in the months October till Febuary (summer)
        \item SQF-RNN is not really fit for this kind of problem, usually uses way more different correlated tracks
        \item SQF-RNN always outperforms DeepAR (assumes Student's \(t\)-distribution)
        \begin{itemize}
            \item[\(\leadsto\)] SQF-RNN better suited for nonparametric tasks like weather forecasting
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{References}
    \printbibliography[heading=none]
\end{frame}

\end{document}