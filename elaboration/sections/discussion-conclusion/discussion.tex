\section{Discussion}
\label{sec:discussion}

In this section, we discuss the results, compare the different 
models and look at potential weaknesses of the competition. 

Looking at Figure \ref{fig:pinball-loss}, we see that NNQF and QRF perform very similarly. 
In the months from October to Febuary, i.e., the summer months since 
Australia is located in the southern hemisphere, we can see that 
the SQF-RNN model performs noticably better than the NNQF and QRF models. 
One explanation could be that the NNQF and QRF models only focus on solar 
radiation but not on total cloud cover.  
In the summer months, the solar energy is influenced in a large part by 
how many clouds are in the sky. Figure \ref{fig:feature-importance} shows us that 
the SQF-RNN model focuses more on total cloud cover. Therefore, 
it performs better in these months. 

When comparing the SQF-RNN and the DeepAR model with 
the Student's \(t\)-distribution in Figure \ref{fig:pinball-loss} and 
Figure \ref{fig:energy-score}, we can see that the DeepAR model 
always performs worse than the SQF-RNN model. 
The nonparametric approach is better than assuming 
that the target variable follows an arbitrary distribution. 
Thus we can conclude that the Student's \(t\)-distribution is not the optimal fit 
and spline quantile functions model the distribution better. 

When looking at the energy score in Figure \ref{fig:energy-score}, 
we can see that the SQF-RNN model outperforms the NNQF model as well as QRF.
One reason why the SQF-RNN works better here is that it takes 
the time series attributes into account because of its RNN structure. 
The NNQF model only uses lag features while the QRF model does not incorporate 
the previous values at all, it only looks at the current time point.

As discussed in Chapter \ref{ch:evaluation}, 
the QRF predictions are capped 
at the top (Figure \ref{fig:predictions-qrf}). 
This is because of the QRF's predictive nature: it generates 
the prediction directly from the previous target data 
and thus cannot predict values 
that are higher than the maximal values of the training set. 
The other two models do not suffer from this because they use 
neural networks to predict the target distribution.

The PIT histograms indicate whether if the forecasting method is probabilistically 
calibrated or if it is over- or underdispersed. If the forecast is not neutrally dispersed, 
it cannot be probabilistically calibrated.
Figure \ref{fig:pit} indicates that the QRF and NNQF model are 
probabilistically calibrated while the PIT histograms of SQF-RNN 
and DeepAR model are skewed and thus their predictive distribution is 
biased in their location.

When we look at the PIT histograms by each hour 
for quantile regression forests in Figure \ref{fig:pit-qrf-by-hour} 
we can observe that the predictions for the night are underdispersed. 
Since the solar plants do not produce energy during the night 
it makes sense to just predict \(0\) for the target variable when there is 
no sunshine.

The NNQF model also suffers a bit from this problem. We can see 
in Figure \ref{fig:pit-nnqf-by-hour} in the time frame 22:00-00:00 that 
there is a high density at \(0\) indicating that the forecast 
was often too high. This can be fixed like in the proposition above.

The hourly PIT histograms for the SQF-RNN and DeepAR model in 
Figure \ref{fig:pit-deepar-by-hour} and Figure \ref{fig:pit-sqf-rnn-by-hour} 
both indicate the same as mentioned above: both predictions are biased 
in their location. One solution would be postprocessing by shifting the 
predictions so that the PITs are approximately symmetric. 

The competition aims to mimic real time solar energy forecasting with 
a \(\SI{24}{\hour}\) forecast horizon. In this scenario, the energy data of 
the previous days would be available. Since the competition organizers want to 
prevent overfitting the data and minimize organizational overhead, 
the target variables are not available for 
the entire month. Since the SQF-RNN model uses the previous time points for the prediction, 
this might be one reason why it does not perform as expected. 
It is the most complicated of the three but has the worst results 
despite taking temporal dependencies into account. 
In order to compare the model fairly with the other competitors, 
we implemented it under competition conditions. A more realistic approach would be 
providing the model with the target data and evaluating it this way.