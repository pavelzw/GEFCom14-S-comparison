\documentclass[10pt,aspectratio=169]{beamer}

\usetheme[progressbar=frametitle]{metropolis}

\include{preamble}

\title{Non-Parametric Machine Learning Models for Solar Energy Forecasting}
% \subtitle{}
\date{16.07.2021}
\author{Pavel Zwerschke}
\institute{Karlsruhe Institute of Technology}
\titlegraphic{\hfill\includegraphics[height=1.5cm]{logos/kitlogo_en_cmyk}}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
    \setbeamertemplate{section in toc}[sections numbered]
    \tableofcontents%[hideallsubsections]
\end{frame}

\begin{frame}{Introduction}
    \begin{itemize}
        \item Solar energy generation is characterized by fluctuations due to uncertainty of the weather
        \item Uncertainty should be quantified through a probabilistic forecast
        \begin{itemize}
            \item Probabilistic forecasts for solar energy generation are underdeveloped
            \item[\(\leadsto\)] We want to compare different machine learning based forecasting models on solar power
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Model descriptions}

\begin{frame}{NNQF}
    \begin{center}
        \includegraphics{plots/nnqf_approach.pdf}
    \end{center}
    \begin{itemize}
        \item Let \(x_1, \ldots, x_n \in \R^D\) be the predictors and \(y_1, \ldots, y_n\in \R\) the target values.
        \item Calculate approximate quantiles of \(y_i\):
        \begin{itemize}
            \item Find \(N\) nearest neighbors of \(x_i\): \(\set{y_{i_1}, \ldots, y_{i_N}}\)
            \item Calculate the empirical quantiles \(y_{(0.01)}, \ldots, y_{(0.99)}\) from \(\set{y_{i_1}, \ldots, y_{i_N}}\)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{NNQF}
    \begin{itemize}
        \item After modification of training set, a data mining technique is used for learning the map \(f(x) = (y_{(0.01)}, \ldots, y_{(0.99)})\).
        \item High correlation of adjacent data points \(\leadsto\) don't just use \(x_i\) for prediction of \(y_i\), but also 
        \(x_{i-1}, \ldots, x_{i-H+1}\)
    \end{itemize}
\end{frame}

\begin{frame}{Advantages of NNQF}
    \begin{itemize}
        \item \(q\) is a free parameter and can be changed to any \(q\in (0,1)\)
        \item the regression technique is not specified, any technique can be used
        \item nearest neighbor calculation only needs to be done once
        \item the original dataset does not need to be saved
    \end{itemize}
\end{frame}

\begin{frame}{QRF}
    \begin{itemize}
        \item Use bagging to produce \(k\) trees from training set \(x_1, \ldots, x_n \in \R^D\) and \(y_1, \ldots, y_n \in \R\)
        \item For \(x\in \R^D\), we want to predict the distribution \(\P(Y | X=x)\)
        \begin{itemize}
            \item Calculate \(\hat{y}_1, \ldots, \hat{y}_k\) from the trees 
            \item Calculate the empirical quantiles \(\hat{y}_{(q)}\) of \(\set{\hat{y}_1, \ldots, \hat{y}_k}\) for any \(q \in (0,1)\)
        \end{itemize}
        \item[\(\leadsto\)] Basically like Random Forests but instead of calculating the mean, you calculate the quantiles of the prediction
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{DeepAR -- Training}
    \begin{center}
        \input{plots/deepar_training}
    \end{center}
    \begin{itemize}
        \item Autoregressive Recurrent Neural Network with probabilistic output
        \item \(x_t\) and \(z_{t-1}\) form with \(\boldsymbol{h}_{t-1}\) the new network output \(\boldsymbol{h}_t\)
        which is used to compute the likelihood \(\P(z_t | \boldsymbol{h}_t)\)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{DeepAR -- Predicting}
    \begin{center}
        \input{plots/deepar_predicting}
    \end{center}
    \begin{itemize}
        \onslide<2->{\item Generate \(\tilde{z}_t \sim \P(\cdot | \boldsymbol{h}_t)\) and use it in the next step as input}
    \end{itemize}
\end{frame}

\begin{frame}{SQF-RNN}
    \begin{itemize}
        \item Output distribution is given by monotonously increasing linear splines: 
        \[ s(x; \gamma, b, d) = \gamma + \sum_{l=0}^L b_l (x - d_l)_+ \]
        \item Arbitrary distribution can be fit \(\leadsto\) no assumption on distribution
    \end{itemize}
\end{frame}

\section{Comparison}

\begin{frame}[fragile]{Pinball loss}
    \renewcommand{\b}[1]{\mathbf{#1}}
    \begin{tabular}{c|cccccc}
        Task & \(4\) & \(5\) & \(6\) & \(7\) & \(8\) & \(9\) \\
        \hline
        NNQF    & \(\b{0.01559}\) & \(\b{0.02091}\) & \(\b{0.01896}\) & \(0.02267\)     & \(0.02330\)     & \(0.02334\)     \\
        QRF     & \(0.01559\)     & \(0.02100\)     & \(0.01983\)     & \(0.02350\)     & \(0.02447\)     & \(0.02370\)     \\
        SQF-RNN & \(0.02581\)     & \(0.03041\)     & \(0.02451\)     & \(\b{0.01895}\) & \(\b{0.01707}\) & \(\b{0.01833}\)
    \end{tabular}
    \begin{tabular}{c|cccccc|c}
        Task & \(10\) & \(11\) & \(12\) & \(13\) & \(14\) & \(15\) & Mean \\
        \hline
        NNQF    & \(0.02333\)     & \(\b{0.02038}\) & \(\b{0.01912}\) & \(\b{0.01673}\) & \(\b{0.01363}\) & \(\b{0.01480}\) & \(\b{0.01940}\) \\
        QRF     & \(0.02483\)     & \(0.02160\)     & \(0.01932\)     & \(0.01694\)     & \(0.01536\)     & \(0.01526\)     & \(0.02015\)     \\
        SQF-RNN & \(\b{0.02002}\) & \(0.02104\)     & \(0.02204\)     & \(0.01684\)     & \(0.01338\)     & \(0.01648\)     & \(0.02041\)
    \end{tabular}
\end{frame}

\begin{frame}[fragile]{Energy score}
    \renewcommand{\b}[1]{\mathbf{#1}}
    \begin{tabular}{c|cccccc}
        Task & \(4\) & \(5\) & \(6\) & \(7\) & \(8\) & \(9\) \\
        \hline
        NNQF    & \(0.30482\)     & \(0.40201\)     & \(0.37712\)     & \(0.42732\)     & \(0.42651\)     & \(0.41919\)     \\
        QRF     & \(\b{0.30431}\) & \(\b{0.40166}\) & \(0.37702\)     & \(0.42787\)     & \(0.42602\)     & \(0.41971\)     \\
        SQF-RNN & \(0.42407\)     & \(0.54152\)     & \(\b{0.37154}\) & \(\b{0.30926}\) & \(\b{0.32410}\) & \(\b{0.33759}\)
    \end{tabular}
    \begin{tabular}{c|cccccc|c}
        Task & \(10\) & \(11\) & \(12\) & \(13\) & \(14\) & \(15\) & Overall \\
        \hline
        NNQF    & \(0.41308\)     & \(0.37499\)     & \(0.36515\)     & \(\b{0.31821}\) & \(0.28263\)     & \(\b{0.30319}\) & \(0.36736\)     \\
        QRF     & \(0.41166\)     & \(0.37581\)     & \(\b{0.36495}\) & \(0.31922\)     & \(\b{0.28258}\) & \(0.30330\)     & \(0.36759\)     \\
        SQF-RNN & \(\b{0.36245}\) & \(\b{0.31466}\) & \(0.39435\)     & \(0.34716\)     & \(0.29842\)     & \(0.35548\)     & \(\b{0.36644}\)
    \end{tabular}
\end{frame}

\begin{frame}{Feature importance}
    \begin{tabular}{c|cccccc}
        Feature & SSRD & STRD & TSR & TP & TCC & Previous time series \\
        \hline
        NNQF    & \(1.2202\) & \(1.0052\) & \(1.0856\) & \(1.0032\) & \(1.0405\) & -- \\
        QRF     & \(1.3365\) & \(1.0214\) & \(1.0199\) & \(1.0020\) & \(1.0025\) & -- \\
        SQF-RNN & \(1.0268\) & \(1.0208\) & \(1.0516\) & \(1.0124\) & \(1.2059\) & \(1.0918\)
    \end{tabular}
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item NNQF is best on average
        \item SQF-RNN is noticably better in the months October till Febuary
        \item NNQF is slightly better than QRF
        \item SQF-RNN \(\succ\) NNQF \(\succ\) QRF in the energy score
        \begin{itemize}
            \item Energy score takes time series attributes more into account, SQF-RNN is better at that due to the RNN structure
        \end{itemize}
        \item SQF-RNN is not really fit for this kind of problem, usually uses way more different correlated tracks
    \end{itemize}
\end{frame}

\end{document}