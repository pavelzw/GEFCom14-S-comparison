\section{Nearest Neighbor Quantile Filters}
\label{sec:implementation-nnqf}

The basic steps for the quantile filters are provided by \Textcite{Ordiano2019} 
on GitHub\footnote{\url{https://github.com/JorgeAngel/nnqf_filter}}. 

Since the NNQF method is only a preprocessing step for the target values, 
we still need to decide which model we want to use for fitting the 
conditional distribution function. 
\Textcite{Ordiano2019} tried fitting each of the \(99\) quantiles 
with a polynomial of maximum degree \(1\) to \(4\) or a multi layer perceptron 
with \(6\) or \(10\) hidden neurons. Since the multi layer perceptron leads to 
noticably better results, we will focus on this regression method. 

Because the data is a time series, timepoints that are close are correlated. 
Therefore, we not only take the predictor value \(x_n \in \R^D\) of time point \(n\) 
but also \(x_{n-1}, \ldots, x_{n-H+1}\) as predictor values, where \(H\) is the number of lags.
All in all, we want to fit a function \(\func{f_q}{\R^{D\times H}}{\R}\), 
where \( f(x_n, \ldots, x_{n-H+1}; \theta_{(q)})\) is the conditional 
\(q\)-quantile of the target value \(Y_n\) and \(\theta_{(q)}\) are the weights 
of the regression model for the \(q\)-quantile.

In order to achieve this lagging, pyWATTS contains a \texttt{Sampler} class
that transforms the data in a way that afterwards, each timepoint contains the 
predictor data of the previous \(H\) timepoints.

\Textcite{Ordiano2019} use separate neural networks with \(6\) or \(10\) hidden nodes for each quantile, 
which is computationally more expensive than training one neural network with 
one hidden layer with \(50\) nodes for \(99\) outputs. 
\todo{Both methods perform approximately the same in evaluation chapter; add numbers to show that} 

Since implementing a model in pyWATTS with a variable number of neural networks is currently 
not easily possible, we will use a single neural network to approximate the quantiles.

In order to avoid quantile crossing, \Textcite{Ordiano2019} postprocess the conditional quantiles:
\[ \hat{y}_{(q)} = \begin{cases}
    \max\set{ f(x; \theta_{(q)}), 0 }, &\text{if } q = 0.01, \\
    \max\set{ f(x; \theta_{(q)}), f(x; \theta_{(q-0.01)}) }, &\text{else.}
\end{cases}\]

In this thesis, we use another approach: \\
We sort all estimated conditional quantiles \(\set{ f(x; \theta_{(q)}) \;|\; q\in \set{0.01, \ldots, 0.99} }\) 
and set \(\hat{y}_{(q)}\) as the \((q\cdot 100)\)-th entry of the sorted list. 
\todo{This results in a noticable performance improvement 
in comparison to taking the maximum. add numbers to show noticable improvement! \(\leadsto\) in evaluation chapter}

After that, the pinball loss is calculated the same way as in the QRF case.

In the NNQF model, the only hyperparameters for the preprocessing part are 
the metric that is used calculating the distances, 
the number of neighbors that should be considered and 
the number of lags \(H\). 
The other hyperparameters depend on the regression model. 
In the case of the multi layer perceptron, the usual hyperparameters like 
hidden layer sizes, activation function, solver and learning rate can be tuned. 
To improve stability, we also use ensemble training at the expense of training and evaluation time.
The parameters after tuning as well as the hyperparameter space 
are shown in Table \ref{table:nnqf-hyperparameters}. 
The resulting losses are all in the range \([0.02, 0.025]\) 
but the best loss was already approximately achieved by the default configuration. 
No noticable performance improvement is visible.

\begin{table}[ht]%
    \caption{NNQF Hyperparameters}
    \label{table:nnqf-hyperparameters}
    \rowcolors{2}{white}{gray!25}
    \centering
    \footnotesize
    \begin{tabular}{lll}
    \toprule \noalign{\smallskip}
    \tableheads Hyperparameter & \tableheads Optimization space & \tableheads Value \\ 
    \midrule
    Number of neighbors & \(\set{50, 100, 150, 200}\)     & \(50\)                      \\
    Distance metric     & --                              & euclidean \(|| \cdot ||_2\) \\
    Number of lags      & \(\set{12, 24, 48, 96}\)        & \(12\)                      \\
    Hidden layer sizes  & \(\set{(50), (50, 50), (100), 
                          (50, 100, 50)}\)                & one layer with \(50\) nodes \\
    Activation function & \(\set{\text{ReLU}, \tanh}\)    & ReLU                        \\
    Solver              & --                              & Adam                        \\
    Learning rate       & \(\set{0.0001,0.001,0.01,0.1}\) & \(0.001\)                   \\
    Ensemble size       & --                              & \(3\)                       \\
    \bottomrule
    \end{tabular}
\end{table}