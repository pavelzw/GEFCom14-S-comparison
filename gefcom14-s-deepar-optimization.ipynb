{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/pavelzw/GEFCom14-S-comparison/blob/main/gefcom14-s-deepar.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/pavelzw/GEFCom14-S-comparison/blob/main/gefcom14-s-deepar.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/pavelzw/GEFCom14-S-comparison/blob/main/gefcom14-s-deepar.ipynb&fileName=gefcom14-s-deepar\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"Downloading GEFCom14-S...\"\n",
    "!rm -R data > /dev/null\n",
    "!wget -O gefcom14.zip https://www.dropbox.com/s/pqenrr2mcvl0hk9/GEFCom2014.zip?dl=0\n",
    "!unzip gefcom14 > /dev/null\n",
    "!rm gefcom14.zip > /dev/null\n",
    "!unzip GEFCom2014\\ Data/GEFCom2014-S_V2.zip > /dev/null\n",
    "!rm -R GEFCom2014\\ Data > /dev/null\n",
    "!mv Solar data > /dev/null\n",
    "!echo \"------------------------------\"\n",
    "!echo \"Downloaded GEFCom14-S in data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pip packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonts\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "task = 2\n",
    "# only use surface solar radiation (169), surface thermal radiation (175) and top net solar radiation (178)\n",
    "predictors = pd.read_csv(f'data/Task {task}/predictors{task}.csv', parse_dates=['TIMESTAMP'])\\\n",
    "    [['ZONEID', 'TIMESTAMP', 'VAR169', 'VAR175', 'VAR178']].set_index('TIMESTAMP')\n",
    "train = pd.read_csv(f'data/Task {task}/train{task}.csv', parse_dates=['TIMESTAMP']).set_index('TIMESTAMP')\n",
    "benchmark = pd.read_csv(f\"data/Task {task-1}/benchmark{'0' + str(task-1) if task-1 < 10 else task}.csv\",\n",
    "                        parse_dates=['TIMESTAMP']).set_index('TIMESTAMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "solar_plants = [train[train['ZONEID'] == i][['POWER']].rename({'POWER': f'ZONEID {i}'}, axis='columns')\n",
    "                for i in [1,2,3]]\n",
    "train_data = pd.concat(solar_plants, axis=1)\n",
    "\n",
    "predictors_categories = [predictors[predictors['ZONEID'] == i][['VAR169', 'VAR175', 'VAR178']]\n",
    "                             .rename({'VAR169': f'SURFACE SOLAR RADIATION {i}',\n",
    "                                      'VAR175': f'SURFACE THERMAL RADIATION {i}',\n",
    "                                      'VAR178': f'TOP NET SOLAR RADIATION {i}'}, axis='columns')\n",
    "                         for i in [1,2,3]]\n",
    "predictor_data = pd.concat(predictors_categories, axis=1)[:'2013-05-01 00:00']\n",
    "\n",
    "# define the parameters of the dataset\n",
    "gefcom14_metadata = {'num_series': 3,\n",
    "                     'num_steps': 24 * (365 + 30),\n",
    "                     'prediction_length': 24 * 30, # 1 month (April)\n",
    "                     'freq': '1H',\n",
    "                     'start': [pd.Timestamp(\"2012-04-01 01:00\", freq='1H') for _ in range(3)]\n",
    "                     }\n",
    "\n",
    "# train_ds\n",
    "targets = [train_data[:-gefcom14_metadata['prediction_length']][f'ZONEID {i}'].values for i in [1,2,3]]\n",
    "starts = gefcom14_metadata['start']\n",
    "features = [predictor_data[:-gefcom14_metadata['prediction_length']][[f'SURFACE SOLAR RADIATION {i}', f'SURFACE THERMAL RADIATION {i}', f'TOP NET SOLAR RADIATION {i}']].values.T for i in [1,2,3]]\n",
    "\n",
    "train_ds = ListDataset([{\n",
    "    FieldName.TARGET: target,\n",
    "    FieldName.START: start,\n",
    "    FieldName.FEAT_DYNAMIC_REAL: fdr,\n",
    "    FieldName.FEAT_STATIC_CAT: [fsc]\n",
    "  } for (target, start, fdr, fsc) in zip(targets, starts, features, [1,2,3])],\n",
    "  freq=gefcom14_metadata['freq'])\n",
    "\n",
    "# test_ds\n",
    "targets = [train_data[f'ZONEID {i}'].values for i in [1,2,3]]\n",
    "starts = gefcom14_metadata['start']\n",
    "features = [predictor_data[[f'SURFACE SOLAR RADIATION {i}', f'SURFACE THERMAL RADIATION {i}', f'TOP NET SOLAR RADIATION {i}']].values.T for i in [1,2,3]]\n",
    "\n",
    "test_ds = ListDataset([{\n",
    "    FieldName.TARGET: target,\n",
    "    FieldName.START: start,\n",
    "    FieldName.FEAT_DYNAMIC_REAL: fdr,\n",
    "    FieldName.FEAT_STATIC_CAT: [fsc]\n",
    "  } for (target, start, fdr, fsc) in zip(targets, starts, features, [1,2,3])],\n",
    "  freq=gefcom14_metadata['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.distribution import PiecewiseLinearOutput\n",
    "\n",
    "def train(epochs=5,\n",
    "          num_pieces=2,\n",
    "          num_cells=40,\n",
    "          num_layers=2,\n",
    "          context_length=None,\n",
    "          cell_type='lstm',\n",
    "          dropoutcell_type='ZoneoutCell',\n",
    "          # available: 'ZoneoutCell', 'RNNZoneoutCell', 'VariationalDropoutCell' or 'VariationalZoneoutCell'\n",
    "          dropout_rate=0.1,\n",
    "          use_feat_dynamic_real=False,\n",
    "          alpha=0.0,\n",
    "          beta=0.0):\n",
    "    estimator = DeepAREstimator(freq=gefcom14_metadata['freq'],\n",
    "                                prediction_length=gefcom14_metadata['prediction_length'],\n",
    "                                num_cells=num_cells,\n",
    "                                num_layers=num_layers,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                context_length=context_length,\n",
    "                                cell_type=cell_type, # lstm, gru available\n",
    "                                dropoutcell_type=dropoutcell_type,\n",
    "                                use_feat_dynamic_real=use_feat_dynamic_real,\n",
    "                                alpha=alpha,\n",
    "                                beta=beta,\n",
    "                                distr_output=PiecewiseLinearOutput(num_pieces=num_pieces), # SQF-RNN\n",
    "                                trainer=Trainer(epochs=epochs))\n",
    "    predictor = estimator.train(train_ds, validation_data=test_ds)\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "def predict(predictor):\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=1000,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    zone_predictions = []\n",
    "    for i, forecast in enumerate(forecasts):\n",
    "        prediction = pd.concat([np.maximum(forecast.quantile_ts(p/100), 0)\n",
    "                                for p in range(1, 100)], axis=1)\\\n",
    "            .rename(columns={p: str((p+1)/100) for p in range(99)})\n",
    "        prediction.insert(0, 'ZONEID', i+1)\n",
    "        prediction.index.name = 'TIMESTAMP'\n",
    "        zone_predictions.append(prediction)\n",
    "\n",
    "    predictions = pd.concat(zone_predictions)\n",
    "    return tss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is the pinnball loss:\n",
    "$$ L(q_a, y) = \\begin{cases}\n",
    "    (1-\\frac{a}{100})(q_a - y), &\\text{if } y < q_a \\\\\n",
    "    \\frac{a}{100}(y-q_a), &\\text{if } y \\geq q_a.\n",
    "\\end{cases} $$\n",
    "\n",
    "The score is then averaged over all target quantiles for all time periods over the forecast horizon and for all zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pinnball_loss(actual, prediction):\n",
    "    actual = actual[...,None]\n",
    "\n",
    "    percentiles = np.empty((actual.shape[0], 99))\n",
    "    for i in range(1, 100):\n",
    "        percentiles[:, i-1] = i\n",
    "    loss = np.where(actual < prediction,\n",
    "                    (1 - percentiles / 100) * (prediction - actual),\n",
    "                    percentiles / 100 * (actual - prediction))\n",
    "    return loss\n",
    "\n",
    "def calculate_loss(tss, predictions):\n",
    "    loss1 = np.mean(pinnball_loss(tss[0].values[-24*30:,0], predictions[predictions['ZONEID'] == 1].drop('ZONEID', axis=1)))\n",
    "    loss2 = np.mean(pinnball_loss(tss[1].values[-24*30:,0], predictions[predictions['ZONEID'] == 2].drop('ZONEID', axis=1)))\n",
    "    loss3 = np.mean(pinnball_loss(tss[2].values[-24*30:,0], predictions[predictions['ZONEID'] == 3].drop('ZONEID', axis=1)))\n",
    "    loss = (loss1 + loss2 + loss3) / 3\n",
    "    print(f\"Loss of task {task}: {loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL\n",
    "\n",
    "def objective(x):\n",
    "    epochs = 5\n",
    "    alpha = 0.0\n",
    "    beta = 0.0\n",
    "    num_pieces, num_cells, num_layers, context_length, \\\n",
    "    cell_type, dropoutcell_type, dropout_rate, use_feat_dynamic_real = x\n",
    "\n",
    "    print(f'Testing with {x}')\n",
    "\n",
    "    try:\n",
    "        predictor = train(epochs, num_pieces, num_cells, num_layers, context_length, cell_type, dropoutcell_type,\n",
    "                          dropout_rate, use_feat_dynamic_real, alpha, beta)\n",
    "        tss, predictions = predict(predictor)\n",
    "        loss = calculate_loss(tss, predictions)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'status': STATUS_OK\n",
    "        }\n",
    "    except UnboundLocalError:\n",
    "        return {\n",
    "            'status': STATUS_FAIL\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "\n",
    "space = [\n",
    "    hp.choice('num_pieces', range(2, 8)),\n",
    "    hp.choice('num_cells', range(20, 100)),\n",
    "    hp.choice('num_layers', range(2, 6)),\n",
    "    hp.choice('context_length', range(24 * 60)),\n",
    "    hp.choice('cell_type', ['lstm', 'gru']),\n",
    "    hp.choice('dropoutcell_type', ['ZoneoutCell', 'RNNZoneoutCell', 'VariationalDropoutCell', 'VariationalZoneoutCell']),\n",
    "    hp.uniform('dropout_rate', 0.01, 0.5),\n",
    "    hp.choice('use_feat_dynamic_real', [False, True])\n",
    "]\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    trials=trials,\n",
    "    max_evals=48)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 30,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 0.017649407492718944, 'status': 'ok'},\n",
       " 'misc': {'tid': 30,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'cell_type': [30],\n",
       "   'context_length': [30],\n",
       "   'dropout_rate': [30],\n",
       "   'dropoutcell_type': [30],\n",
       "   'num_cells': [30],\n",
       "   'num_layers': [30],\n",
       "   'num_pieces': [30],\n",
       "   'use_feat_dynamic_real': [30]},\n",
       "  'vals': {'cell_type': [1],\n",
       "   'context_length': [1419],\n",
       "   'dropout_rate': [0.20388833937807294],\n",
       "   'dropoutcell_type': [0],\n",
       "   'num_cells': [62],\n",
       "   'num_layers': [3],\n",
       "   'num_pieces': [1],\n",
       "   'use_feat_dynamic_real': [0]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2021, 4, 23, 7, 59, 37, 62000),\n",
       " 'refresh_time': datetime.datetime(2021, 4, 23, 9, 11, 40, 624000)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 1,\n",
       " 'context_length': 1419,\n",
       " 'dropout_rate': 0.20388833937807294,\n",
       " 'dropoutcell_type': 0,\n",
       " 'num_cells': 62,\n",
       " 'num_layers': 3,\n",
       " 'num_pieces': 1,\n",
       " 'use_feat_dynamic_real': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.argmin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
