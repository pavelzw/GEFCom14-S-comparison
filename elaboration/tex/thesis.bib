% Encoding: UTF-8

@Article{Hong2016,
  author    = {Tao Hong and Pierre Pinson and Shu Fan and Hamidreza Zareipour and Alberto Troccoli and Rob J. Hyndman},
  journal   = {International Journal of Forecasting},
  title     = {Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond},
  year      = {2016},
  month     = jul,
  number    = {3},
  pages     = {896--913},
  volume    = {32},
  doi       = {10.1016/j.ijforecast.2016.02.001},
  publisher = {Elsevier {BV}},
}

@Article{Ordiano2019,
  author        = {Jorge Ángel González Ordiano and Lutz Gröll and Ralf Mikut and Veit Hagenmeyer},
  journal       = {International Journal of Forecasting},
  volume        = {36},
  pages         = {310--323},
  title         = {Probabilistic Energy Forecasting using Quantile Regressions based on a new Nearest Neighbors Quantile Filter},
  year          = {2019},
  month         = mar,
  abstract      = {Parametric quantile regressions are a useful tool for creating probabilistic energy forecasts. Nonetheless, since classical quantile regressions are trained using a non-differentiable cost function, their creation using complex data mining techniques (e.g., artificial neural networks) may be complicated. This article presents a method that uses a new nearest neighbors quantile filter to obtain quantile regressions independently of the utilized data mining technique and without the non-differentiable cost function. Thereafter, a validation of the presented method using the dataset of the Global Energy Forecasting Competition of 2014 is undertaken. The results show that the presented method is able to solve the competition's task with a similar accuracy and in a similar time as the competition's winner, but requiring a much less powerful computer. This property may be relevant in an online forecasting service for which the fast computation of probabilistic forecasts using not so powerful machines is required.},
  archiveprefix = {arXiv},
  doi           = {10.1016/j.ijforecast.2019.06.003},
  eprint        = {1903.07390},
  file          = {:http\://arxiv.org/pdf/1903.07390v1:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{Gasthaus2019,
  title         = "{Probabilistic Forecasting with Spline Quantile Function RNNs}",
  author        = {Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
  booktitle     = {The 22nd International Conference on Artificial Intelligence and Statistics},
  pages         = {1901--1910},
  year          = {2019},
  abstract      = {In this paper, we propose a flexible method for probabilistic modeling with conditional 
  quantile functions using monotonic regression splines. The shape of the spline is parameterized by 
  a neural network whose parameters are learned by minimizing the continuous ranked probability score. 
  Within this framework, we propose a method for probabilistic time series forecasting, which combines 
  the modeling capacity of recurrent neural networks with the flexibility of a spline-based representation 
  of the output distribution. Unlike methods based on parametric probability density functions 
  and maximum likelihood estimation, the proposed method can flexibly adapt to different output distributions 
  without manual intervention. We empirically demonstrate the effectiveness 
  of the approach on synthetic and real-world data sets.},
  editor        = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume        = {89},
  series        = {Proceedings of Machine Learning Research},
  publisher     = {PMLR},
  pdf           = {http://proceedings.mlr.press/v89/gasthaus19a/gasthaus19a.pdf},
  url           = {http://proceedings.mlr.press/v89/gasthaus19a.html},
}

@Article{Huang2016,
  author    = {Jing Huang and Matthew Perry},
  journal   = {International Journal of Forecasting},
  title     = {A semi-empirical approach using gradient boosting and k-nearest neighbors regression for {GEFCom}2014 probabilistic solar power forecasting},
  year      = {2016},
  month     = jul,
  number    = {3},
  pages     = {1081--1086},
  volume    = {32},
  abstract  = {The aim of this work is to produce probabilistic forecasts of solar power for the Global
  Energy Forecasting Competition 2014 (GEFCom2014). The task involves predicting the
  outputs from three solar farms at an hourly resolution using data from the ECMWF
  numerical weather prediction model.
  The annual cycle of solar radiation and power is modelled using a low-pass filter built
  using a Fourier transformation. The diurnal cycle is handled by fitting separate models for
  each hour of the day with the positive solar radiation. A model for simulating PV power
  production, taking the effect of temperature into account, is also included.
  The forecasting methods were gradient boosting for the deterministic forecasting of
  solar power and k-nearest neighbors regression for estimating prediction intervals in
  order to provide probabilistic forecasts. A cross-validation strategy, splitting the data into
  monthly folds, was employed for comparing the performances of alternative methods and
  in an attempt to avoid overfitting issues.},
  comment   = {gang gang},
  doi       = {10.1016/j.ijforecast.2015.11.002},
  publisher = {Elsevier {BV}},
}

@Article{Nagy2016,
  author    = {G{\'{a}}bor I. Nagy and Gerg{\H{o}} Barta and S{\'{a}}ndor Kazi and Gyula Borb{\'{e}}ly and G{\'{a}}bor Simon},
  journal   = {International Journal of Forecasting},
  title     = {{GEFCom}2014: Probabilistic solar and wind power forecasting using a generalized additive tree ensemble approach},
  year      = {2016},
  month     = jul,
  number    = {3},
  pages     = {1087--1093},
  volume    = {32},
  abstract  = {We investigate the probabilistic forecasting of solar and wind power generation in
  connection with the Global Energy Forecasting Competition 2014.We use a voted ensemble
  of a quantile regression forest model and a stacked random forest – gradient boosting
  decision tree model to predict the probability distribution. The raw probabilities thus
  obtained need to be post-processed using isotonic regression in order to conform to
  the monotonic-increase attribute of probability distributions. The results show a great
  performance in terms of the weighted pinball loss, with the model achieving second place
  on the final competition leaderboard.},
  comment   = {dmlab},
  doi       = {10.1016/j.ijforecast.2015.11.013},
  publisher = {Elsevier {BV}},
}

@Article{Juban2016,
  author    = {Romain Juban and Henrik Ohlsson and Mehdi Maasoumy and Louis Poirier and J. Zico Kolter},
  journal   = {International Journal of Forecasting},
  title     = {A multiple quantile regression approach to the wind, solar, and price tracks of {GEFCom}2014},
  year      = {2016},
  month     = jul,
  number    = {3},
  pages     = {1094--1102},
  volume    = {32},
  abstract  = {This paper proposes a generic framework for probabilistic energy forecasting, and discusses
  the application of the method to several tracks in the 2014 Global Energy Forecasting
  Competition (GEFCom2014). The proposed method uses a multiple quantile regression
  approach to predict a full distribution over possible future energy outcomes, uses the
  alternating direction method of multipliers to solve the optimization problems resulting
  from this quantile regression formulation efficiently, and uses a radial basis function
  network to capture the non-linear dependencies on the input data. For the GEFCom2014
  competition, the approach proved general enough to obtain one of the top five ranks in
  three tracks, solar, wind, and price forecasting, and it was also ranked seventh in the final
  load forecasting track.},
  comment   = {C3 green team},
  doi       = {10.1016/j.ijforecast.2015.12.002},
  publisher = {Elsevier {BV}},
}

@Article{Jordan2019,
  author    = {Alexander Jordan and Fabian Krüger and Sebastian Lerch},
  journal   = {Journal of Statistical Software},
  title     = {Evaluating Probabilistic Forecasts with {scoringRules}},
  year      = {2019},
  number    = {12},
  volume    = {90},
  comment   = {Energy score, section 5},
  doi       = {10.18637/jss.v090.i12},
  publisher = {Foundation for Open Access Statistic},
}

@Article{Rasul2020,
  author        = {Kashif Rasul and Abdul-Saboor Sheikh and Ingmar Schuster and Urs Bergmann and Roland Vollgraf},
  title         = {Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows},
  year          = {2020},
  month         = feb,
  abstract      = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multivariate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multivariate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
  archiveprefix = {arXiv},
  comment       = {Zalando},
  eprint        = {2002.06103},
  file          = {:http\://arxiv.org/pdf/2002.06103v3:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Hong2016a,
  author    = {Tao Hong and Shu Fan},
  journal   = {International Journal of Forecasting},
  title     = {Probabilistic electric load forecasting: A tutorial review},
  year      = {2016},
  month     = jul,
  number    = {3},
  pages     = {914--938},
  volume    = {32},
  doi       = {10.1016/j.ijforecast.2015.11.011},
  publisher = {Elsevier {BV}},
}

@Article{Salinas2017,
  author        = {David Salinas and Valentin Flunkert and Jan Gasthaus},
  title         = {DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks},
  journal       = {International Journal of Forecasting},
  volume        = {36},
  pages         = {1181--1191},
  year          = {2017},
  month         = apr,
  abstract      = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  eprint        = {1704.04110},
  file          = {:http\://arxiv.org/pdf/1704.04110v3:PDF},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
}

@Article{Meinshausen2006,
  author   = {Nicolai Meinshausen},
  journal  = {Journal of Machine Learning Research},
  title    = {Quantile regression forests},
  year     = {2006},
  number   = {Jun},
  volume   = {7},
  pages    = {983--999},
  abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have
  since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional
  mean of a response variable. It is shown here that random forests provide information
  about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a
  generalisation of random forests. Quantile regression forests give a non-parametric and
  accurate way of estimating conditional quantiles for high-dimensional predictor variables.
  The algorithm is shown to be consistent. Numerical examples suggest that the algorithm
  is competitive in terms of predictive power.},
  url      = {https://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf},
}

@Article{Heidrich2021,
  author        = {Benedikt Heidrich and Andreas Bartschat and Marian Turowski and Oliver Neumann and Kaleb Phipps and Stefan Meisenbacher and Kai Schmieder and Nicole Ludwig and Ralf Mikut and Veit Hagenmeyer},
  title         = {pyWATTS: Python Workflow Automation Tool for Time Series},
  year          = {2021},
  month         = jun,
  abstract      = {Time series data are fundamental for a variety of applications, ranging from financial markets to energy systems. Due to their importance, the number and complexity of tools and methods used for time series analysis is constantly increasing. However, due to unclear APIs and a lack of documentation, researchers struggle to integrate them into their research projects and replicate results. Additionally, in time series analysis there exist many repetitive tasks, which are often re-implemented for each project, unnecessarily costing time. To solve these problems we present \texttt{pyWATTS}, an open-source Python-based package that is a non-sequential workflow automation tool for the analysis of time series data. pyWATTS includes modules with clearly defined interfaces to enable seamless integration of new or existing methods, subpipelining to easily reproduce repetitive tasks, load and save functionality to simply replicate results, and native support for key Python machine learning libraries such as scikit-learn, PyTorch, and Keras.},
  archiveprefix = {arXiv},
  eprint        = {2106.10157},
  file          = {:http\://arxiv.org/pdf/2106.10157v1:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@InProceedings{Bergstra2011,
  author    = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Algorithms for Hyper-Parameter Optimization},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {24},
  url       = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
}

@Article{Ma2015,
  author    = {Xuejun Ma and Xiaoqun He and Xiaokang Shi},
  journal   = {Journal of Applied Statistics},
  title     = {A variant of {K} nearest neighbor quantile regression},
  year      = {2015},
  month     = {aug},
  number    = {3},
  pages     = {526--537},
  volume    = {43},
  doi       = {10.1080/02664763.2015.1070807},
  publisher = {Informa {UK} Limited},
}

@Article{Rueschendorf2009,
  author    = {Ludger Rüschendorf},
  journal   = {Journal of Statistical Planning and Inference},
  title     = {On the distributional transform, Sklar{\textquotesingle}s theorem, and the empirical copula process},
  year      = {2009},
  month     = {nov},
  number    = {11},
  pages     = {3921--3927},
  volume    = {139},
  doi       = {10.1016/j.jspi.2009.05.030},
  publisher = {Elsevier {BV}},
}

@Article{Gneiting2014,
  author    = {Tilmann Gneiting and Matthias Katzfuss},
  journal   = {Annual Review of Statistics and Its Application},
  title     = {Probabilistic Forecasting},
  year      = {2014},
  month     = {jan},
  number    = {1},
  pages     = {125--151},
  volume    = {1},
  doi       = {10.1146/annurev-statistics-062713-085831},
  publisher = {Annual Reviews},
}

@Article{Breiman2001,
  author    = {Leo Breiman},
  journal   = {Machine Learning},
  year      = {2001},
  number    = {1},
  pages     = {5--32},
  volume    = {45},
  doi       = {10.1023/a:1010933404324},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Bergstra2013,
  author    = {James Bergstra and Daniel Yamins and David Cox},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  title     = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
  year      = {2013},
  address   = {Atlanta, Georgia, USA},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  month     = {17--19 Jun},
  number    = {1},
  pages     = {115--123},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {28},
  abstract  = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.     In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included.  A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric.  Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.},
  pdf       = {http://proceedings.mlr.press/v28/bergstra13.pdf},
  url       = {https://proceedings.mlr.press/v28/bergstra13.html},
}

@Article{Alexandrov2019,
  author        = {Alexander Alexandrov and Konstantinos Benidis and Michael Bohlke-Schneider and Valentin Flunkert and Jan Gasthaus and Tim Januschowski and Danielle C. Maddix and Syama Rangapuram and David Salinas and Jasper Schulz and Lorenzo Stella and Ali Caner Türkmen and Yuyang Wang},
  title         = {GluonTS: Probabilistic Time Series Models in Python},
  year          = {2019},
  month         = jun,
  abstract      = {We introduce Gluon Time Series (GluonTS, available at https://gluon-ts.mxnet.io), a library for deep-learning-based time series modeling. GluonTS simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy.},
  archiveprefix = {arXiv},
  eprint        = {1906.05264},
  file          = {:http\://arxiv.org/pdf/1906.05264v2:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Gneiting2007,
  author    = {Tilmann Gneiting and Adrian E Raftery},
  journal   = {Journal of the American Statistical Association},
  title     = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  year      = {2007},
  month     = {mar},
  number    = {477},
  pages     = {359--378},
  volume    = {102},
  doi       = {10.1198/016214506000001437},
  publisher = {Informa {UK} Limited},
}

@Article{Meer2018,
  author    = {D.W. van der Meer and J. Wid{\'{e}}n and J. Munkhammar},
  journal   = {Renewable and Sustainable Energy Reviews},
  title     = {Review on probabilistic forecasting of photovoltaic power production and electricity consumption},
  year      = {2018},
  month     = {jan},
  pages     = {1484--1512},
  volume    = {81},
  doi       = {10.1016/j.rser.2017.05.212},
  publisher = {Elsevier {BV}},
}

@Article{Hong2020,
  author    = {Tao Hong and Pierre Pinson and Yi Wang and Rafal Weron and Dazhi Yang and Hamidreza Zareipour},
  journal   = {{IEEE} Open Access Journal of Power and Energy},
  title     = {Energy Forecasting: A Review and Outlook},
  year      = {2020},
  pages     = {376--388},
  volume    = {7},
  doi       = {10.1109/oajpe.2020.3029979},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Comment{jabref-meta: databaseType:bibtex;}
