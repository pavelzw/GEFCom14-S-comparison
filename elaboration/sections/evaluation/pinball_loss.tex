\section{Pinball Loss}
\label{sec:elaboration-pinball-loss}

As stated in section \ref{sec:pinball-loss-explanation}, the pinball loss is 
used to determine the performance of the different models. 
It is calculated by taking the average over all pinball losses for each time 
point and zone in the dataset. 

Table \ref{table:pinball-loss} and Figure \ref{fig:pinball-loss} show the 
losses of the models for task 4 to task 15 (July 2013 to June 2014). 
We can see that the QRF and NNQF model perform similarly and that the 
SQF-RNN model performs better than the other two during the months from October to Febuary.
Another thing to note is that the DeepAR model always performs worse than the SQF-RNN model.

\begin{table}[ht]%
    \footnotesize
    \hspace*{25pt} % make kind of centering
    \begin{minipage}{\textwidth}
    \renewcommand{\b}[1]{\textbf{#1}}
    \rowcolors{2}{white}{gray!25}
    \begin{tabular}{c|cccccc}
        \toprule \noalign{\smallskip}
        Task & \(4\) & \(5\) & \(6\) & \(7\) & \(8\) & \(9\) \\
        \midrule
        QRF     & \(\b{0.0146}\) & \(\b{0.0202}\) & \(\b{0.0188}\) & \(0.0225\)     & \(0.0226\)     & \(0.0221\)     \\
        NNQF    & \(0.0156\)     & \(0.0209\)     & \(0.0190\)     & \(0.0227\)     & \(0.0233\)     & \(0.0233\)     \\
        SQF-RNN & \(0.0258\)     & \(0.0304\)     & \(0.0245\)     & \(\b{0.0190}\) & \(\b{0.0171}\) & \(\b{0.0183}\) \\
        DeepAR  & \(0.0263\)     & \(0.0364\)     & \(0.0274\)     & \(0.0196\)     & \(0.0258\)     & \(0.0229\)     \\
        \bottomrule
    \end{tabular}
    \vspace*{1em} \\
    \rowcolors{2}{white}{gray!25}
    \begin{tabular}{c|cccccc|c}
        \toprule \noalign{\smallskip}
        Task & \(10\) & \(11\) & \(12\) & \(13\) & \(14\) & \(15\) & Mean \\
        \midrule
        QRF     & \(0.0223\)     & \(\b{0.0201}\) & \(\b{0.0182}\) & \(\b{0.0156}\) & \(\b{0.0136}\) & \(\b{0.0140}\) & \(\b{0.0187}\) \\
        NNQF    & \(0.0233\)     & \(0.0204\)     & \(0.0191\)     & \(0.0167\)     & \(0.0136\)     & \(0.0148\)     & \(0.0194\)     \\
        SQF-RNN & \(\b{0.0200}\) & \(0.0210\)     & \(0.0220\)     & \(0.0168\)     & \(0.0134\)     & \(0.0165\)     & \(0.0204\)     \\
        DeepAR  & \(0.0232\)     & \(0.0261\)     & \(0.0242\)     & \(0.0228\)     & \(0.0187\)     & \(0.0189\)     & \(0.0244\)     \\
        \bottomrule
    \end{tabular}
    \end{minipage}

    \caption[Pinball loss]{Pinball loss. 
    Each task is one month in the training period. 
    Task 4 represents July 2013, Task 5 August 2013, etc. up until June 2014.
    The pinball loss is calculated by averaging 
    over all pinball losses for each time point and zone.}
    \label{table:pinball-loss}
\end{table}

\begin{figure}[ht]
    \centering
    \input{plots/pinball_loss}
    \caption[Pinball loss]{Pinball loss. 
    This graph plots the losses of the models for each month of the dataset competition.}
    \label{fig:pinball-loss}
\end{figure}

As described in section \ref{sec:implementation-nnqf}, 
we only use one neural network with more hidden nodes instead of 
separate neural networks for each node in the second step of the NNQF model. 
The pinball loss for training the model with \(99\) different neural networks 
is \(0.01998\), so the version with one neural network performs approximately the 
same while being noticably faster. 
Another modification to the original algorithm proposed by \Textcite{Ordiano2019} 
is sorting the predicted quantiles instead of taking the maximum as described in 
section \ref{sec:implementation-nnqf}. The average pinball loss over all tasks for the second is 
\(0.02742\), so sorting the quantiles instead of taking the maximum of the previous quantile 
results in a noticable performance improvement.