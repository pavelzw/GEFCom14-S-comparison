\section{Nearest Neighbor Quantile Filters}
\label{sec:nnqf}

\Textcite{Ordiano2019} proposed a method for probabilistic 
energy forecasting using quantile regression based on a \gls{nnqf}. 
The method works as follows: first, the training set is modified 
by using the Nearest Neigbor Quantile Filters so that 
the training data directly represents a probabilistic distribution. 
Now, a regression model like an artificial neural network can 
train on this modified data set and learn the quantile function.

Let \(x_n \in \R^D\) be the \(n\)-th predictor 
and \(y_n \in \R\) the \(n\)-th value of the time series, \(1 \leq n \leq N\). 
The NNQF first searches the \(k\) nearest neighbors of \(x_n\). 
Let \(J \subset \set{1, \ldots, N}\) be the indices of 
those nearest neighbors. 
The probabilistic distribution of \(y_n\) can be approximated 
by calculating the empirical quantile \(\tilde{y}_{(q),n}\) of 
\(\set{y_n \;|\; n\in J}\) for each \(q \in \set{0.01, \ldots, 0.99}\). 

After repeating this procedure for each entry in the time series, 
we get vectors 
\[ \tilde{y}_{(q)} = \begin{pmatrix}
    \tilde{y}_{(q), 1} \\ 
    \vdots \\
    \tilde{y}_{(q), N}
\end{pmatrix} \]
that form the modified training set combined with the 
predictors \(X = (x_1, \ldots, x_N)\).

With the modified training set, one can now train the regression model 
for each quantile. Common examples are polynomial regression or 
artificial neural networks. 

This method has three main advantages: 
\begin{enumerate}
    \item \(q\) is a free parameter and can be changed to any \(q \in (0,1)\),
    \item the technique for the quantile regression is not specified, 
    any technique can be used,
    \item the calculation of the nearest neighbors and the modified 
    training set only needs to be done once, you can save time when 
    using multiple quantile regressions. Also, the original dataset 
    does not need to be saved afterwards.
\end{enumerate}

In comparison to most other \(k\)-Nearest Neighbors quantile 
regression techniques, the nearest neighbors are only calculated once 
and then the regression model is trained on the modified training data. 
A regular \(k\)-Nearest Neighbor Quantile Regression algorithm 
calculates the nearest neighbors every time when a forecast is conducted 
which is computationally more expensive in the long run.