\section{Nearest Neighbor Quantile Filters}
\label{sec:implementation-nnqf}

Implementations of the basic steps for the quantile filters are provided by \Textcite{Ordiano2019} 
on GitHub\footnote{\url{https://github.com/JorgeAngel/nnqf_filter}}. 

Since the NNQF method is only a preprocessing step for the target values, 
we still need to decide on a model for fitting the 
conditional distribution function. 
\Textcite{Ordiano2019} tried fitting each of the \(99\) quantiles 
with a polynomial of maximum degree \(1\) to \(4\) or a multi layer perceptron 
with \(6\) or \(10\) hidden neurons. Since the multi layer perceptron leads to 
noticably better results, we will focus on this regression method. 

Because the data is a time series, time points that are close are correlated. 
Therefore, we not only take the predictor value \(x_n \in \R^D\) of time point \(n\) 
but also \(x_{n-1}, \ldots, x_{n-H+1}\) as predictor values, where \(H\) is the number of lags.
All in all, we want to fit a function \(\func{f_q}{\R^{D\times H}}{\R}\), 
where \( f(x_n, \ldots, x_{n-H+1}; \theta_{(q)})\) is the conditional 
\(q\)-quantile of the target value \(Y_n\) and \(\theta_{(q)}\) are the weights 
of the regression model for the \(q\)-quantile.
In order to achieve this lagging, pyWATTS contains a \texttt{Sampler} class
that transforms the data so that each time point contains the 
predictor data of the previous \(H\) time points.

\Textcite{Ordiano2019} use separate neural networks with \(6\) or \(10\) hidden nodes for each quantile, 
which is computationally more expensive than training one neural network with 
one hidden layer with \(50\) nodes for \(99\) outputs. 

Since implementing a model in pyWATTS with a variable number of neural networks is currently 
not supported natively, we will use a single neural network to approximate the quantiles.

In order to avoid quantile crossing, \Textcite{Ordiano2019} postprocess the conditional quantiles:
\[ \hat{y}_{(q)} = \begin{cases}
    \max\set{ f(x; \theta_{(q)}), 0 }, &\text{if } q = 0.01, \\
    \max\set{ f(x; \theta_{(q)}), f(x; \theta_{(q-0.01)}) }, &\text{else.}
\end{cases}\]

In this thesis, we use another approach: 
We sort all estimated conditional quantiles \(\set{ f(x; \theta_{(q)}) \;|\; q\in \set{0.01, \ldots, 0.99} }\) 
and set \(\hat{y}_{(q)}\) as the \((q\cdot 100)\)-th entry of the sorted list. 

After that, the pinball loss is calculated the same way as in the QRF case.

In the NNQF model, the only hyperparameters for the preprocessing part are 
the metric that is used for calculating the distances, 
the number of neighbors that should be considered and 
the number of lags \(H\). 
The other hyperparameters effect the regression model. 
In the case of the multi layer perceptron, the usual hyperparameters like 
hidden layer sizes, activation function, solver and learning rate can be tuned. 
To improve stability, we also use ensemble training at the expense of training and evaluation time.
The parameters after tuning as well as the hyperparameter space 
are shown in Table \ref{table:nnqf-hyperparameters}. 
The resulting losses are all in the range \([0.02, 0.025]\) 
but the best loss was already approximately achieved by the default configuration. 
No noticable performance improvement is visible.

\begin{table}[ht]%
    \caption{NNQF Hyperparameters}
    \label{table:nnqf-hyperparameters}
    \rowcolors{2}{white}{gray!25}
    \centering
    \footnotesize
    \begin{tabular}{lll}
    \toprule \noalign{\smallskip}
    \tableheads Hyperparameter & \tableheads Optimization space & \tableheads Final value \\ 
    \midrule
    Number of neighbors & \(\set{50, 100, 150, 200}\)     & \(50\)                      \\
    Distance metric     & --                              & euclidean \(|| \cdot ||_2\) \\
    Number of lags      & \(\set{12, 24, 48, 96}\)        & \(12\)                      \\
    Hidden layer sizes  & \(\set{(50), (50, 50), (100), 
                          (50, 100, 50)}\)                & one layer with \(50\) nodes \\
    Activation function & \(\set{\text{ReLU}, \tanh}\)    & ReLU                        \\
    Solver              & --                              & Adam                        \\
    Learning rate       & \(\set{0.0001,0.001,0.01,0.1}\) & \(0.001\)                   \\
    Ensemble size       & --                              & \(3\)                       \\
    \bottomrule
    \end{tabular}
\end{table}