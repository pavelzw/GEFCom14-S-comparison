\section{Spline Quantile Function RNNs}
\label{sec:implementation-sqf-rnn}

The implementation for the SQF-RNN model is provided by 
\Textcite{Gasthaus2019} on GitHub\footnote{\url{https://github.com/awslabs/gluon-ts}}.

The key difference between the SQF-RNN model and the DeepAR model from 
\Textcite{Salinas2017} is that the DeepAR implementation uses a 
parametric distribution and optimizes the distribution parameters 
based on the maximum likelihood estimation. 
In the DeepAR default implementation, a Student's \(t\)-distribution is used. 
In the SQF-RNN case, spline quantile functions are used and their parameters 
are optimized based on the CRPS. 
For complex problems, the specification of a probabilistic distribution 
that fits the data is often not trivial. 
As the CRPS is closely related to the pinball loss (see \ref{ch:crps}), 
this helps in the GEFCom14 problem since it directly minimizes the given metric.

The data preprocessing steps are the same as in the QRF case. 
After preprocessing, the trainig data is converted into a \texttt{ListDataset} and fitted with the 
\texttt{DeepAREstimator} class. The frequency of the model is set to one hour and 
prediction length to 28, 30 or 31 days since the task is to predict one full month. 
In order to use quantile splines with three parts as the output distribution, 
we need to set the distribution output of the model to \texttt{PiecewiseLinearOutput(num\_pieces=3)}, 
i.e., linear spline quantile functions. 
The default implementation of DeepAR does not use the predictors but only uses the previous time data. 
In order to use the predictors \(x_1, \ldots, x_n \in \R^D\), 
we need to change the value of \texttt{use\_feat\_dynamic\_real}
to \texttt{True} or else the model will ignore them. 

After training for seven epochs, the model with the data that is available from the months before, 
we need to predict the upcoming month. This is done by calling the \texttt{predict()} 
method from the predictor that we got after training.
Then, the quantiles are calculated from the results and used to 
calculate the pinball loss.

In order to get better and more consistent results, 
seven independent models are trained simultaneously and in the predicition step 
and the average of all models is used as output.
\todo{how much performance improvement? \(\leadsto\) evaluation chapter}

Because the DeepAR model is similar to a recurrent neural network, 
the tunable hyperparameters also look similar. 
The context length -- e.g. the number of steps to unroll the RNN 
before computing predictions -- can be tuned, 
the number of RNN layers as well as the number of RNN cells for each layer. 
The cell type of the RNN can be either an LSTM or a gated recurrent unit (GRU) cell.
Another hyperparameter is the dropout regularization: 
the dropout rate and the dropout cell type can be tuned 
with available cell types being \texttt{ZoneoutCell}, 
\texttt{RNNZoneoutCell}, \texttt{VariationalDropoutCell} 
and \texttt{VariationalZoneoutCell}.
For the output distribution, the number of spline pieces can be adjusted.
For the training part, the usual hyperparameters like the number of epochs, batch size, 
learning rate, learning rate decay, patience, gradient clip and weight decay can be tuned.
The parameters after tuning and their corresponding optimization spaces 
are shown in Table \ref{table:sqf-rnn-hyperparameters}.
The resulting losses were all in the range \([0.02, 0.025]\) but the best loss 
was already achieved by the default configuration. No noticable performance improvement is visible.

\begin{table}[h!]%
    \caption{SQF-RNN Hyperparameters}
    \label{table:sqf-rnn-hyperparameters}
    \rowcolors{2}{white}{gray!25}
    \centering
    \footnotesize
    \begin{tabular}{lll}
    \toprule \noalign{\smallskip}
    \tableheads Hyperparameter & \tableheads Optimization space & \tableheads Final value \\ 
    \midrule
    Context length          & \set{\(1\) week, 
                              \(1\) month, \(2\) months} & \(1\) month \\
    RNN layers              & \(\set{2, 3, 4}\)          & \(2\) \\
    RNN cells per layer     & \(\set{20, 40, 60, 100}\)  & \(40\) \\
    RNN cell type           & \set{LSTM, GRU}            & LSTM \\
    Dropout rate            & \(\set{0.01, 0.1, 0.5}\)   & \(0.1\) \\
    Dropout Cell Type       & --                         & \texttt{ZoneoutCell} \\
    Number of spline pieces & \(\set{2, 3, 4, 5}\)       & \(3\) \\
    Number of epochs        & --                         & \(7\) \\
    Batch size              & --                         & \(32\) \\
    Learning rate           & \(\set{0.0005, 0.001, 0.01, 
                              0.05, 0.1, 0.2, 0.5}\)     & \(0.001\) \\
    Learning rate decay     & \(\set{0.3, 0.5, 0.7}\)    & \(0.5\) \\
    Patience                & --                         & \(10\) \\
    Gradient clip           & --                         & \(10\) \\
    Weight decay            & --                         & \(10^{-8}\) \\
    Ensemble size           & --                         & \(3\) \\
    \bottomrule
    \end{tabular}
\end{table}